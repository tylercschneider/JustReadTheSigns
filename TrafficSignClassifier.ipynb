{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# German Traffic Sign Recognition Classifier\n",
    "### SDCND\n",
    "#### By: Tyler Schneider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 0: Load The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pickled data\n",
    "import pickle\n",
    "\n",
    "# File Locations\n",
    "training_file = \"train.p\"\n",
    "validation_file= \"valid.p\"\n",
    "testing_file = \"test.p\"\n",
    "\n",
    "# Open Files and Store in variables\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(validation_file, mode='rb') as f:\n",
    "    valid = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "# Separate Feature and Label data using key pairs\n",
    "X_train, y_train = train['features'], train['labels']\n",
    "X_valid, y_valid = valid['features'], valid['labels']\n",
    "X_test, y_test = test['features'], test['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Dataset Summary & Exploration\n",
    "\n",
    "The pickled data is a dictionary with 4 key/value pairs:\n",
    "\n",
    "- `'features'` is a 4D array containing raw pixel data of the traffic sign images, (num examples, width, height, channels).\n",
    "- `'labels'` is a 1D array containing the label/class id of the traffic sign. The file `signnames.csv` contains id -> name mappings for each id.\n",
    "- `'sizes'` is a list containing tuples, (width, height) representing the original width and height the image.\n",
    "- `'coords'` is a list containing tuples, (x1, y1, x2, y2) representing coordinates of a bounding box around the sign in the image. **THESE COORDINATES ASSUME THE ORIGINAL IMAGE. THE PICKLED DATA CONTAINS RESIZED VERSIONS (32 by 32) OF THESE IMAGES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of training examples = 34799\n",
      "Number of testing examples = 12630\n",
      "Image data shape = (32, 32, 3)\n",
      "Number of classes = 43\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import for image data manipulation\n",
    "import cv2\n",
    "# Print some summary statistics about dataset and individual entries\n",
    "\n",
    "\n",
    "# Number of training examples\n",
    "n_train = len(X_train)\n",
    "\n",
    "# Number of validation examples\n",
    "n_validation = len(X_valid)\n",
    "\n",
    "# Number of test examples\n",
    "n_test = len(X_test)\n",
    "\n",
    "# Shape of traffic img\n",
    "image_shape = X_train[0].shape\n",
    "\n",
    "# Unique classes/labels there are in the dataset.\n",
    "n_classes = len(set(y_train))\n",
    "\n",
    "print()\n",
    "print(\"Number of training examples =\", n_train)\n",
    "print(\"Number of testing examples =\", n_test)\n",
    "print(\"Image data shape =\", image_shape)\n",
    "print(\"Number of classes =\", n_classes)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x128f64748>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get an idea about the images in dataset by printing one.\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "image = X_train[20000]\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Step 2: Design and Test a Model Architecture\n",
    "\n",
    "Training and testing model on the [German Traffic Sign Dataset](http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset) to recognize and identify signs.\n",
    "\n",
    "Heavily influenced by LeNet.\n",
    "\n",
    "expect a validation set accuracy of about 0.89. \n",
    "To meet specifications, at least 0.93."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34799, 32, 32, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# shuffle training data to make sure model receives diverse examples so it trains better\n",
    "X_train, y_train = shuffle(X_train, y_train)\n",
    "\n",
    "# preprocess function for any array of images\n",
    "def preprocess_image(x):\n",
    "    \n",
    "    # CONVERT TO GREY\n",
    "    gray = np.sum(x/3, axis=3, keepdims=True)\n",
    "    \n",
    "    # NORMALIZE so Values are -1 to 1\n",
    "    normal = (gray-128)/128\n",
    "    \n",
    "    return normal\n",
    "\n",
    "# preprocess the different sets of data\n",
    "X_train = preprocess_image(X_train)\n",
    "X_valid = preprocess_image(X_valid)\n",
    "X_test = preprocess_image(X_test)\n",
    "\n",
    "# check data to make sure it is in the expected format\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARCHITECTURE\n",
    "from tensorflow.contrib.layers import flatten\n",
    "\n",
    "# Same Architecture as LeNet except output is 43 instead of LeNet's 10\n",
    "def LeNets_Child(x):    \n",
    "    # Arguments used for tf.truncated_normal, randomly defines variables for the weights and biases for each layer\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    \n",
    "    ##########################################\n",
    "    # LAYER 1\n",
    "    # Convolutional. Input = 32x32x1. Output = 28x28x6.\n",
    "    conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 1, 6), mean = mu, stddev = sigma))\n",
    "    conv1_b = tf.Variable(tf.zeros(6))\n",
    "    conv1   = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n",
    "\n",
    "    # Activation.\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "\n",
    "    # POOL. Input = 28x28x6. Output = 14x14x6.\n",
    "    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    \n",
    "    ##########################################\n",
    "    # LAYER 2\n",
    "    # Convolutional. Output = 10x10x16.\n",
    "    conv2_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 6, 16), mean = mu, stddev = sigma))\n",
    "    conv2_b = tf.Variable(tf.zeros(16))\n",
    "    conv2   = tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding='VALID') + conv2_b\n",
    "    \n",
    "    # Activation.\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "\n",
    "    # POOL. Input = 10x10x16. Output = 5x5x16.\n",
    "    conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    # Flatten. Input = 5x5x16. Output = 400.\n",
    "    fc0   = flatten(conv2)\n",
    "    \n",
    "    ###########################################\n",
    "    # LAYER 3\n",
    "    # Fully Connected. Input = 400. Output = 120.\n",
    "    fc1_W = tf.Variable(tf.truncated_normal(shape=(400, 120), mean = mu, stddev = sigma))\n",
    "    fc1_b = tf.Variable(tf.zeros(120))\n",
    "    fc1   = tf.matmul(fc0, fc1_W) + fc1_b\n",
    "    \n",
    "    # Activation.\n",
    "    fc1    = tf.nn.relu(fc1)\n",
    "    \n",
    "    ############################################\n",
    "    # Layer 4\n",
    "    # Fully Connected. Input = 120. Output = 84.\n",
    "    fc2_W  = tf.Variable(tf.truncated_normal(shape=(120, 84), mean = mu, stddev = sigma))\n",
    "    fc2_b  = tf.Variable(tf.zeros(84))\n",
    "    fc2    = tf.matmul(fc1, fc2_W) + fc2_b\n",
    "    \n",
    "    # Activation.\n",
    "    fc2    = tf.nn.relu(fc2)\n",
    "\n",
    "    ############################################\n",
    "    # LAYER 5\n",
    "    #Fully Connected. Input = 84. Output = 43.\n",
    "    fc3_W  = tf.Variable(tf.truncated_normal(shape=(84, 43), mean = mu, stddev = sigma))\n",
    "    fc3_b  = tf.Variable(tf.zeros(43))\n",
    "    logits = tf.matmul(fc2, fc3_W) + fc3_b\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Validate and Test the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A validation set can be used to assess how well the model is performing. A low accuracy on the training and validation\n",
    "sets imply underfitting. A high accuracy on the training set but low accuracy on the validation set implies overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, (None, 32, 32, 1))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "one_hot_y = tf.one_hot(y, 43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "\n",
    "rate = 0.001\n",
    "\n",
    "logits = LeNets_Child(x)\n",
    "\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y, logits=logits)\n",
    "\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "\n",
    "training_operation = optimizer.minimize(loss_operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## TRAIN THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "\n",
      "EPOCH 1 ...\n",
      "Validation Accuracy = 0.739\n",
      "\n",
      "EPOCH 2 ...\n",
      "Validation Accuracy = 0.846\n",
      "\n",
      "EPOCH 3 ...\n",
      "Validation Accuracy = 0.894\n",
      "\n",
      "EPOCH 4 ...\n",
      "Validation Accuracy = 0.907\n",
      "\n",
      "EPOCH 5 ...\n",
      "Validation Accuracy = 0.902\n",
      "\n",
      "EPOCH 6 ...\n",
      "Validation Accuracy = 0.927\n",
      "\n",
      "EPOCH 7 ...\n",
      "Validation Accuracy = 0.913\n",
      "\n",
      "EPOCH 8 ...\n",
      "Validation Accuracy = 0.904\n",
      "\n",
      "EPOCH 9 ...\n",
      "Validation Accuracy = 0.908\n",
      "\n",
      "EPOCH 10 ...\n",
      "Validation Accuracy = 0.924\n",
      "\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_train)\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    print()\n",
    "    for i in range(EPOCHS):\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "       \n",
    "        validation_accuracy = evaluate(X_valid, y_valid)\n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "        print()\n",
    "        \n",
    "    saver.save(sess, './lenetschild')\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Test a Model on New Images\n",
    "\n",
    "To give yourself more insight into how your model is working, download at least five pictures of German traffic signs from the web and use your model to predict the traffic sign type.\n",
    "\n",
    "You may find `signnames.csv` useful as it contains mappings from the class id (integer) to the actual sign name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Output the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of testing examples = 12630\n",
      "Image data shape = (32, 32, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find Images to Use from German Traffic Sign Data Set\n",
    "# Load Images from File Directory\n",
    "# Plot Them Here\n",
    "\n",
    "# Number of test examples\n",
    "n_test = len(X_test)\n",
    "\n",
    "# Ensure that data is in correct format\n",
    "image_shape = X_test[0].shape\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"Number of testing examples =\", n_test)\n",
    "print(\"Image data shape =\", image_shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the Sign Type for Each Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 0.909\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver2 = tf.train.import_meta_graph('./lenetschild.meta')\n",
    "    saver2.restore(sess, \"./lenetschild\")\n",
    "    test_accuracy = evaluate(X_test, y_test)\n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pic #1\n",
      "(32, 32, 3)\n",
      "\n",
      "Pic #2\n",
      "(32, 32, 3)\n",
      "\n",
      "Pic #3\n",
      "(32, 32, 3)\n",
      "\n",
      "Pic #4\n",
      "(32, 32, 3)\n",
      "\n",
      "Pic #5\n",
      "(32, 32, 3)\n",
      "\n",
      "(5, 32, 32, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "numNewPics = 5\n",
    "newPics = []\n",
    "plt.figure(figsize=(1,5))\n",
    "\n",
    "# Pull new pics in and process so they can run thru model\n",
    "for i in range(1,6):\n",
    "    # pull\n",
    "    pic = cv2.imread('{}.jpg'.format(i), 1)\n",
    "    plt.imshow(pic)\n",
    "    \n",
    "    # Force pixels to fit 32x32\n",
    "    # this could potentially be better if I pre cropped the photo but surprisingly\n",
    "    # this line will take any size photo and turn it into 32x32\n",
    "    pic = cv2.resize(pic, (32,32))\n",
    "    print(\"Pic #{}\".format(i))\n",
    "    # ensure it was sized correctly\n",
    "    print(pic.shape)\n",
    "    print()\n",
    "    newPics.append(pic)\n",
    "    \n",
    "# My guesses for the \"correct\" guess for each pic. \n",
    "# Found in google search...hopefully correct! Not 100% positive of my ability here \n",
    "correctGuess = [1,38,9,25,17] \n",
    "\n",
    "newPics = np.asarray(newPics)\n",
    "print(newPics.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Pic Accuracy = 0.600\n"
     ]
    }
   ],
   "source": [
    "# Transform images into the right format for model\n",
    "newPics = preprocess_image(newPics)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver2 = tf.train.import_meta_graph('./lenetschild.meta')\n",
    "    saver2.restore(sess, \"./lenetschild\")\n",
    "    test_accuracy = evaluate(newPics, correctGuess)\n",
    "    print(\"New Pic Accuracy = {:.3f}\".format(test_accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACCURACY = 60%\n",
    "### 3 out of 5  \n",
    "### pretty satisfying to reach this point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOW...\n",
    "## A look under the hood.\n",
    "## what was the model thinking for these different pics\n",
    "### Output Top 5 Softmax Probabilities For Each Image Found on the Web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************\n",
      "PIC #1\n",
      "********************************************\n",
      "Correct: 1\n",
      "Guess: 1\n",
      "\n",
      "PROBABILITIES\n",
      "Sign 1 -> 0.999\n",
      "Sign 0 -> 0.001\n",
      "Sign 31 -> 0.001\n",
      "Sign 40 -> 0.000\n",
      "Sign 2 -> 0.000\n",
      "\n",
      "********************************************\n",
      "PIC #2\n",
      "********************************************\n",
      "Correct: 38\n",
      "Guess: 12\n",
      "\n",
      "PROBABILITIES\n",
      "Sign 12 -> 0.995\n",
      "Sign 9 -> 0.005\n",
      "Sign 23 -> 0.000\n",
      "Sign 16 -> 0.000\n",
      "Sign 10 -> 0.000\n",
      "\n",
      "********************************************\n",
      "PIC #3\n",
      "********************************************\n",
      "Correct: 9\n",
      "Guess: 37\n",
      "\n",
      "PROBABILITIES\n",
      "Sign 37 -> 0.969\n",
      "Sign 40 -> 0.025\n",
      "Sign 26 -> 0.006\n",
      "Sign 12 -> 0.000\n",
      "Sign 24 -> 0.000\n",
      "\n",
      "********************************************\n",
      "PIC #4\n",
      "********************************************\n",
      "Correct: 25\n",
      "Guess: 25\n",
      "\n",
      "PROBABILITIES\n",
      "Sign 25 -> 1.000\n",
      "Sign 11 -> 0.000\n",
      "Sign 19 -> 0.000\n",
      "Sign 21 -> 0.000\n",
      "Sign 1 -> 0.000\n",
      "\n",
      "********************************************\n",
      "PIC #5\n",
      "********************************************\n",
      "Correct: 17\n",
      "Guess: 17\n",
      "\n",
      "PROBABILITIES\n",
      "Sign 17 -> 1.000\n",
      "Sign 14 -> 0.000\n",
      "Sign 12 -> 0.000\n",
      "Sign 9 -> 0.000\n",
      "Sign 0 -> 0.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
    "    \n",
    "    predictions = sess.run(logits, feed_dict={x:newPics})\n",
    "    probs = sess.run(tf.nn.softmax(predictions))\n",
    "    top_k_v, top_k_i=sess.run(tf.nn.top_k(probs,k=43))\n",
    "    \n",
    "    for i in range(0,5):\n",
    "        print(\"********************************************\")\n",
    "        print(\"PIC #{}\".format(i+1))\n",
    "        print(\"********************************************\")\n",
    "        print(\"Correct: {0}\".format(correctGuess[i]))\n",
    "        print(\"Guess: {0}\".format(top_k_i[i][0]))\n",
    "        print()\n",
    "        print(\"PROBABILITIES\")\n",
    "        for j in range(0,5):\n",
    "            print(\"Sign {0} -> {1:.3f}\".format(top_k_i[i][j],top_k_v[i][j]))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4 (Optional): Visualize the Neural Network's State with Test Images\n",
    "\n",
    "\n",
    "\n",
    "<figure>\n",
    " <img src=\"visualize_cnn.png\" width=\"380\" alt=\"Combined Image\" />\n",
    " <figcaption>\n",
    " <p></p> \n",
    " <p style=\"text-align: center;\"> Your output should look something like this (above)</p> \n",
    " </figcaption>\n",
    "</figure>\n",
    " <p></p> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Visualize your network's feature maps here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n",
    "# image_input: the test image being fed into the network to produce the feature maps\n",
    "# tf_activation: should be a tf variable name used during your training procedure that represents the calculated state of a specific weight layer\n",
    "# activation_min/max: can be used to view the activation contrast in more detail, by default matplot sets min and max to the actual min and max values of the output\n",
    "# plt_num: used to plot out multiple different weight feature map sets on the same block, just extend the plt number for each new feature map entry\n",
    "\n",
    "def outputFeatureMap(image_input, tf_activation, activation_min=-1, activation_max=-1 ,plt_num=1):\n",
    "    # Here make sure to preprocess your image_input in a way your network expects\n",
    "    # with size, normalization, ect if needed\n",
    "    # image_input =\n",
    "    # Note: x should be the same name as your network's tensorflow data placeholder variable\n",
    "    # If you get an error tf_activation is not defined it may be having trouble accessing the variable from inside a function\n",
    "    activation = tf_activation.eval(session=sess,feed_dict={x : image_input})\n",
    "    featuremaps = activation.shape[3]\n",
    "    plt.figure(plt_num, figsize=(15,15))\n",
    "    for featuremap in range(featuremaps):\n",
    "        plt.subplot(6,8, featuremap+1) # sets the number of feature maps to show on each row and column\n",
    "        plt.title('FeatureMap ' + str(featuremap)) # displays the feature map number\n",
    "        if activation_min != -1 & activation_max != -1:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", vmin =activation_min, vmax=activation_max, cmap=\"gray\")\n",
    "        elif activation_max != -1:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", vmax=activation_max, cmap=\"gray\")\n",
    "        elif activation_min !=-1:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", vmin=activation_min, cmap=\"gray\")\n",
    "        else:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", cmap=\"gray\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
